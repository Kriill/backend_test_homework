{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kriill/backend_test_homework/blob/master/homeworks/hw04_overfitting/hw_fmnist_overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDsVMGiVgSq2"
      },
      "source": [
        "## Переобучение нейронных сетей и борьба с ним\n",
        "\n",
        "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), https://t.me/girafe_ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3isBRG6PgSq6"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "# __________end of block__________\n",
        "\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wROIVXpo4vGx"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "def args_and_kwargs(*args, **kwargs):\n",
        "    return args, kwargs\n",
        "\n",
        "def parse_pytorch_model(model_str):\n",
        "    def parse_layer(layer_str):\n",
        "        layer_name, params = layer_str.split(\"(\", 1)\n",
        "        layer_info = {\"type\": layer_name.strip()}\n",
        "        params_template = layer_str.replace(layer_name, \"args_and_kwargs\")\n",
        "\n",
        "        param_dict = {}\n",
        "        if len(params):\n",
        "            args, kwargs = eval(params_template)\n",
        "            if len(args) or len(kwargs):\n",
        "                param_dict[\"args\"] = args\n",
        "                for name, value in kwargs.items():\n",
        "                    param_dict[name] = value\n",
        "        layer_info[\"parameters\"] = param_dict\n",
        "        return layer_info\n",
        "\n",
        "    model_dict = {}\n",
        "    lines = model_str.splitlines()\n",
        "    model_name = lines[0].strip(\"()\")\n",
        "    model_dict[\"model_name\"] = model_name\n",
        "    model_dict[\"layers\"] = []\n",
        "\n",
        "    layer_regex = re.compile(r\"\\((\\d+)\\): (.+)\")\n",
        "    for line in lines[1:]:\n",
        "        line = line.strip()\n",
        "        match = layer_regex.match(line)\n",
        "        if match:\n",
        "            index, layer = match.groups()\n",
        "            model_dict[\"layers\"].append({\"index\": int(index), \"layer\": parse_layer(layer)})\n",
        "    return model_dict\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0RViEC-C4vGx"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "def get_predictions(model, eval_data, step=10):\n",
        "\n",
        "    predicted_labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, len(eval_data), step):\n",
        "            y_predicted = model(eval_data[idx : idx + step].to(device))\n",
        "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
        "\n",
        "    predicted_labels = torch.cat(predicted_labels)\n",
        "    predicted_labels = \",\".join([str(x.item()) for x in list(predicted_labels)])\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "def get_accuracy(model, data_loader):\n",
        "    predicted_labels = []\n",
        "    real_labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            y_predicted = model(batch[0].to(device))\n",
        "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
        "            real_labels.append(batch[1])\n",
        "\n",
        "    predicted_labels = torch.cat(predicted_labels)\n",
        "    real_labels = torch.cat(real_labels)\n",
        "    accuracy_score = (predicted_labels == real_labels).type(torch.FloatTensor).mean()\n",
        "    return accuracy_score\n",
        "\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTeF60QR4vGy"
      },
      "source": [
        "Загрузите файл `hw_overfitting_data_dict.npy` (ссылка есть на странице с заданием), он понадобится для генерации посылок. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1PQfNkeE4vGy",
        "outputId": "addf17f3-6440-4f31-99e8-08facedeb9bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-21 11:06:28--  https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/girafe-ai/ml-course/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict [following]\n",
            "--2025-04-21 11:06:29--  https://raw.githubusercontent.com/girafe-ai/ml-course/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6272446 (6.0M) [application/octet-stream]\n",
            "Saving to: ‘hw_overfitting_data_dict.npy’\n",
            "\n",
            "hw_overfitting_data 100%[===================>]   5.98M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-21 11:06:29 (98.5 MB/s) - ‘hw_overfitting_data_dict.npy’ saved [6272446/6272446]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict -O hw_overfitting_data_dict.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fV-Oz9pI4vGy"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_overfitting_data_dict.npy\"\n",
        "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeA6Q5-CgSq7"
      },
      "source": [
        "### Задача №1 (уже решённая): Создание и обучение модели (Separation)\n",
        "Вы уже решали эту задачу ранее, так что сейчас просто воспроизведите своё решение. Оно понадобится вам в дальнейших шагах.\n",
        "__Ваша первая задача всё та же: реализовать весь пайплан обучения модели и добиться качества $\\geq 88.5\\%$ на тестовой выборке.__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zXZ2llB94vGy"
      },
      "outputs": [],
      "source": [
        "CUDA_DEVICE_ID = 0  # change if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nPG1KbQAgl8b"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "device = (\n",
        "    torch.device(f\"cuda:{CUDA_DEVICE_ID}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        ")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "aYcL28OsgSq8",
        "outputId": "f7e9474b-0961-4d17-f866-731a285df6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 13.1MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 210kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.86MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 10.8MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Image label: 9')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "train_fmnist_data = FashionMNIST(\n",
        "    \".\", train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
        ")\n",
        "test_fmnist_data = FashionMNIST(\n",
        "    \".\", train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
        ")\n",
        "\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_fmnist_data, batch_size=32, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_fmnist_data, batch_size=32, shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "random_batch = next(iter(train_data_loader))\n",
        "_image, _label = random_batch[0][0], random_batch[1][0]\n",
        "plt.figure()\n",
        "plt.imshow(_image.reshape(28, 28))\n",
        "plt.title(f\"Image label: {_label}\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6jWRv1rgSq8"
      },
      "source": [
        "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 88.5% accuracy.\n",
        "\n",
        "__Внимание, ваша модель должна быть представлена именно переменной `model_task_1`. На вход ей должен приходить тензор размерностью (1, 28, 28).__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BcyEFX-RgSq8"
      },
      "outputs": [],
      "source": [
        "# Creating model instance\n",
        "model_task_1 = None\n",
        "# your code here\n",
        "\n",
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        # Сверточная часть\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        # Полносвязная часть\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Если вход имеет размер [batch_size, 784], преобразуем в [batch_size, 1, 28, 28]\n",
        "        if x.shape[-1] == 784:\n",
        "            x = x.view(-1, 1, 28, 28)\n",
        "\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model_task_1 = MNISTModel() # .to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAoLV4dkoy5M"
      },
      "source": [
        "Не забудьте перенести модель на выбранный `device`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Xas9SIXDoxvZ",
        "outputId": "d689510f-bd88-48f7-d3e2-c8c591ec2da5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MNISTModel(\n",
              "  (conv_layers): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc_layers): Sequential(\n",
              "    (0): Linear(in_features=3136, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model_task_1.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pLRWysggSq9"
      },
      "source": [
        "Локальные тесты для проверки вашей модели доступны ниже:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qMQzo1ggSq9",
        "outputId": "84421b8f-ea22-4631-f4ec-0c3e261ddea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Everything seems fine!\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert model_task_1 is not None, \"Please, use `model_task_1` variable to store your model\"\n",
        "\n",
        "try:\n",
        "    x = random_batch[0].to(device)\n",
        "    y = random_batch[1].to(device)\n",
        "\n",
        "    # compute outputs given inputs, both are variables\n",
        "    y_predicted = model_task_1(x)\n",
        "except Exception as e:\n",
        "    print(\"Something is wrong with the model\")\n",
        "    raise e\n",
        "\n",
        "\n",
        "assert y_predicted.shape[-1] == 10, \"Model should predict 10 logits/probas\"\n",
        "\n",
        "print(\"Everything seems fine!\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suRmIPwIgSq9"
      },
      "source": [
        "Настройте параметры модели на обучающей выборке. Также рекомендуем поработать с `learning rate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YJnU14bdnZa_",
        "outputId": "beb590b5-0d7e-4533-98b9-9433c620e601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ac9e2a43d188>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_task_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-07b7ef0b5a58>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_task_1.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model_task_1.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in train_data_loader:\n",
        "        images, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model_task_1(images.view(-1, 784))\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_data_loader):.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zce7gt1gSq-"
      },
      "source": [
        "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usswrWYOgSq-"
      },
      "source": [
        "Оценим качество классификации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xua3TVZHgSq-"
      },
      "outputs": [],
      "source": [
        "train_acc_task_1 = get_accuracy(model_task_1, train_data_loader)\n",
        "print(f\"Neural network accuracy on train set: {train_acc_task_1:3.5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9KEKXBxgSq-"
      },
      "outputs": [],
      "source": [
        "test_acc_task_1 = get_accuracy(model_task_1, test_data_loader)\n",
        "print(f\"Neural network accuracy on test set: {test_acc_task_1:3.5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oyhmMobgSq_"
      },
      "source": [
        "Проверка, что необходимые пороги пройдены:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAIrURCEgSq_"
      },
      "outputs": [],
      "source": [
        "assert test_acc_task_1 >= 0.885, \"Train accuracy is below 0.885 threshold\"\n",
        "assert (\n",
        "    train_acc_task_1 >= 0.905\n",
        "), \"Test accuracy is below 0.905 while test accuracy is fine. We recommend to check your model and data flow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJQyDJOP4vG0"
      },
      "source": [
        "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBc7qrtv4vG0"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_overfitting_data_dict.npy\"\n",
        "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "loaded_data_dict = np.load(\"hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
        "\n",
        "submission_dict = {\n",
        "    \"train_predictions_task_1\": get_predictions(\n",
        "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
        "    ),\n",
        "    \"test_predictions_task_1\": get_predictions(\n",
        "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
        "    ),\n",
        "    \"model_task_1\": parse_pytorch_model(str(model_task_1)),\n",
        "}\n",
        "\n",
        "with open(\"submission_dict_task_1.json\", \"w\") as iofile:\n",
        "    json.dump(submission_dict, iofile)\n",
        "print(\"File saved to `submission_dict_task_1.json`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-yb4PvC4vG0"
      },
      "source": [
        "### Задача №2: Переобучение (Initiation)\n",
        "Продолжим работу с набором данных [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). Теперь ваша задача продемонстрировать переобучение модели на обучающей выборке. Достаточно показать, что точность классификации (не только функция потерь!) на тестовой выборке значительно отстает от обучающей.\n",
        "\n",
        "Обращаем ваше внимание, в задаче №3 вам придется починить данную модель (минимизировать эффект переобучения) с помощью механизмов регуляризации, поэтому не переусердствуйте!\n",
        "\n",
        "__Ваша вторая задача: реализовать используя пайплан обучения модели продемонстрировать переобучения модели на обучающей выборке.__\n",
        "\n",
        "Код для обучения модели вы можете переиспользовать. Далее присутствует лишь несколько тестов, которые помогут вам проверить свое решение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5ahSVud4vG0"
      },
      "source": [
        "Обращаем внимание, вам необходимо использовать переменную `model_task_2` для хранение модели во второй задаче.\n",
        "\n",
        "Не используйте `Dropout` и `BatchNorm` в этой задаче"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OVERFITModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OVERFITModel, self).__init__()\n",
        "        self.input_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.shape[-1] == 784:\n",
        "            x = x.view(-1, 1, 28, 28)\n",
        "\n",
        "        x = self.input_layer(x)  # Здесь будем переобучать модель. Блок можно выкинуть.\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "j3s1M5NT_pFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KM_UrE14vG0"
      },
      "outputs": [],
      "source": [
        "# Creating model instance\n",
        "model_task_2 = OVERFITModel()\n",
        "# your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIqqMkCA4vG0"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_task_1.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model_task_1.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in train_data_loader:\n",
        "        images, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model_task_1(images.view(-1, 784))\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_data_loader):.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWodFegc4vG0"
      },
      "source": [
        "Проверка архитектуры:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K11PMia4vG0"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "layers_task_2 = []\n",
        "for element in parse_pytorch_model(str(model_task_2)).get(\"layers\", []):\n",
        "    layer_name = element[\"layer\"][\"type\"]\n",
        "    assert \"dropout\" not in layer_name.lower(), \"Do not use Dropout in Task 2!\"\n",
        "    assert \"batchnorm\" not in layer_name.lower(), \"Do not use BatchNorm in Task 2!\"\n",
        "    layers_task_2.append(layer_name)\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niB4hD4y4vG1"
      },
      "source": [
        "Оценим качество классификации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF8IyeDL4vG1"
      },
      "outputs": [],
      "source": [
        "train_acc_task_2 = get_accuracy(model_task_2, train_data_loader)\n",
        "print(f\"Neural network accuracy on train set: {train_acc_task_2:3.5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKs_ybRt4vG1"
      },
      "outputs": [],
      "source": [
        "test_acc_task_2 = get_accuracy(model_task_2, test_data_loader)\n",
        "print(f\"Neural network accuracy on test set: {test_acc_task_2:3.5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79uEQmKR4vG1"
      },
      "source": [
        "Проверка, что переобучение присутствует:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3__7lVHy4vG1"
      },
      "outputs": [],
      "source": [
        "assert train_acc_task_2 >= test_acc_task_2, \"Train accuracy must be higher than task accuracy\"\n",
        "assert train_acc_task_2 >= 0.88, \"Train accuracy must be higher than 0.88\"\n",
        "assert (\n",
        "    train_acc_task_2 - test_acc_task_2 >= 0.04\n",
        "), \"Test accuracy should be at least 0.04 lower that train.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAlTkyk14vG1"
      },
      "source": [
        "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_2`.\n",
        "\n",
        "Также предполагается, что в переменной `submission_dict` уже содержатся результаты задачи №1. Если их там нет, загрузите их из сохраненного файла в переменную перед запуском следующей ячейки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMjwG4h64vHC"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_overfitting_data_dict.npy\"\n",
        "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "loaded_data_dict = np.load(\"hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
        "\n",
        "submission_dict.update(\n",
        "    {\n",
        "        \"train_predictions_task_2\": get_predictions(\n",
        "            model_task_2, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
        "        ),\n",
        "        \"test_predictions_task_2\": get_predictions(\n",
        "            model_task_2, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
        "        ),\n",
        "        \"model_task_2\": parse_pytorch_model(str(model_task_2)),\n",
        "    }\n",
        ")\n",
        "\n",
        "with open(\"submission_dict_tasks_1_and_2.json\", \"w\") as iofile:\n",
        "    json.dump(submission_dict, iofile)\n",
        "print(\"File saved to `submission_dict_tasks_1_and_2.json`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Px1_wi4vHD"
      },
      "source": [
        "### Задача №3: Исправление модели (Return)\n",
        "Все так же работаем с [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). Наконец, ваша задача исправить ~~ошибки прошлого~~ переобучение модели, построенной в задаче 2. Достаточно добиться расхождения между точностью классификации на обучающей и тестовой выборках не превышающего 0.015 (т.е. полутора процентов).\n",
        "\n",
        "Обращаем ваше внимание, архитектура модели в задаче №3 не должна существенно отличаться от задачи №2! Вы можете использовать Batchnorm, Dropout, уменьшить размерность промежуточных представлений, обратиться к аугментации данных, но вы не можете использовать меньшее количество слоёв.\n",
        "\n",
        "__Ваша третья и финальная задача: исправить модель и/или процесс обучения, дабы справиться с переобучением.__\n",
        "\n",
        "Код для обучения модели вы можете переиспользовать. Далее присутствует лишь несколько тестов, которые помогут вам проверить свое решение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9_UxN8u4vHD"
      },
      "source": [
        "Обращаем внимание, вам необходимо использовать переменную `model_task_3` для хранение модели во второй задаче.\n",
        "\n",
        "Также код ниже будет обращаться к переменной `layers_task_2`, инициализируйте её, если она не определена."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3d9iztN4vHD"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert (\n",
        "    layers_task_2 is not None\n",
        "), \"Initializa layers_task_2 vairable which contains list of layers in task 2 model\"\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNI41DDp4vHD"
      },
      "outputs": [],
      "source": [
        "model_task_3 = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtZ-dGFB4vHD"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_zlK8UY4vHD"
      },
      "source": [
        "Проверка архитектуры:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VETyPntP4vHD"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "layers_task_3 = []\n",
        "for element in parse_pytorch_model(str(model_task_3)).get(\"layers\", []):\n",
        "    layer_name = element[\"layer\"][\"type\"]\n",
        "    layers_task_3.append(layer_name)\n",
        "\n",
        "\n",
        "idx = 0\n",
        "for model_3_layer in layers_task_3:\n",
        "    model_2_layer = layers_task_2[idx]\n",
        "    if \"dropout\" not in model_3_layer.lower() and \"batchnorm\" not in model_3_layer.lower():\n",
        "        assert (\n",
        "            model_3_layer == model_2_layer\n",
        "        ), \"Models in tasks 2 and 3 must share the architecture except for Dropout and BatchNorm!\"\n",
        "        idx += 1\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_bnAncF4vHD"
      },
      "source": [
        "Оценим качество классификации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y89DuORi4vHD"
      },
      "outputs": [],
      "source": [
        "train_acc_task_3 = get_accuracy(model_task_3, train_data_loader)\n",
        "print(f\"Neural network accuracy on train set: {train_acc_task_3:3.5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYPL-Uq24vHD"
      },
      "outputs": [],
      "source": [
        "test_acc_task_3 = get_accuracy(model_task_3, test_data_loader)\n",
        "print(f\"Neural network accuracy on test set: {test_acc_task_3:3.5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4YFND7I4vHD"
      },
      "source": [
        "Проверка, что переобучение присутствует:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooP2oRQ_4vHD"
      },
      "outputs": [],
      "source": [
        "assert train_acc_task_3 >= 0.88, \"Train accuracy must be higher than 0.88\"\n",
        "assert train_acc_task_3 >= 0.865, \"Test accuracy must be higher than 0.865\"\n",
        "assert (\n",
        "    train_acc_task_3 - test_acc_task_3 <= 0.015\n",
        "), \"Test accuracy should not be lower that train more than by 0.015\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srUx8asW4vHD"
      },
      "source": [
        "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_3`.\n",
        "\n",
        "Также предполагается, что в переменной `submission_dict` уже содержатся результаты задач №1 и №2. Если их там нет, загрузите их из сохраненных файлов перед запуском следующей ячейки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrTBnURL4vHD"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_overfitting_data_dict.npy\"\n",
        "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "loaded_data_dict = np.load(\"hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
        "\n",
        "submission_dict.update(\n",
        "    {\n",
        "        \"train_predictions_task_3\": get_predictions(\n",
        "            model_task_3, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
        "        ),\n",
        "        \"test_predictions_task_3\": get_predictions(\n",
        "            model_task_3, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
        "        ),\n",
        "        \"model_task_3\": parse_pytorch_model(str(model_task_3)),\n",
        "    }\n",
        ")\n",
        "\n",
        "with open(\"submission_dict_final.json\", \"w\") as iofile:\n",
        "    json.dump(submission_dict, iofile)\n",
        "print(\"File saved to `submission_dict_final.json`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xai8JL3tgSq_"
      },
      "source": [
        "### Сдача задания\n",
        "Сдайте сгенерированные файлы в соответствующие задачи в соревновании, а именно:\n",
        "* `submission_dict_tasks_1_and_2.json` в задачу Initiation\n",
        "* `submission_dict_final.json` в задачу Return.\n",
        "\n",
        "\n",
        "`submission_dict_task_1.json` сдавать не нужно, он уже был сдан ранее."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtWnYAN_gSrA"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}